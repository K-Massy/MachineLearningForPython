{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting mnist/train-images-idx3-ubyte.gz\n",
      "Extracting mnist/train-labels-idx1-ubyte.gz\n",
      "Extracting mnist/t10k-images-idx3-ubyte.gz\n",
      "Extracting mnist/t10k-labels-idx1-ubyte.gz\n",
      "step= 0 loss= 531.587 acc= 0.1179\n",
      "step= 100 loss= 53.7318 acc= 0.8213\n",
      "step= 200 loss= 32.7404 acc= 0.9019\n",
      "step= 300 loss= 23.6931 acc= 0.9246\n",
      "step= 400 loss= 37.4094 acc= 0.9345\n",
      "step= 500 loss= 16.2145 acc= 0.9445\n",
      "step= 600 loss= 20.0372 acc= 0.9491\n",
      "step= 700 loss= 8.73677 acc= 0.9512\n",
      "step= 800 loss= 19.2576 acc= 0.9526\n",
      "step= 900 loss= 13.8845 acc= 0.9567\n",
      "step= 1000 loss= 8.8835 acc= 0.9598\n",
      "step= 1100 loss= 3.19575 acc= 0.965\n",
      "step= 1200 loss= 7.22396 acc= 0.9642\n",
      "step= 1300 loss= 2.46884 acc= 0.966\n",
      "step= 1400 loss= 3.73675 acc= 0.9689\n",
      "step= 1500 loss= 6.34133 acc= 0.9677\n",
      "step= 1600 loss= 8.81936 acc= 0.9695\n",
      "step= 1700 loss= 21.4133 acc= 0.9719\n",
      "step= 1800 loss= 10.7832 acc= 0.9707\n",
      "step= 1900 loss= 2.6238 acc= 0.9728\n",
      "step= 2000 loss= 2.83861 acc= 0.9744\n",
      "step= 2100 loss= 4.14944 acc= 0.9753\n",
      "step= 2200 loss= 4.57132 acc= 0.977\n",
      "step= 2300 loss= 3.58848 acc= 0.9743\n",
      "step= 2400 loss= 3.78886 acc= 0.9766\n",
      "step= 2500 loss= 5.57071 acc= 0.978\n",
      "step= 2600 loss= 4.42462 acc= 0.9789\n",
      "step= 2700 loss= 8.24457 acc= 0.9803\n",
      "step= 2800 loss= 2.38115 acc= 0.98\n",
      "step= 2900 loss= 1.77677 acc= 0.9818\n",
      "step= 3000 loss= 10.429 acc= 0.9779\n",
      "step= 3100 loss= 2.87606 acc= 0.9808\n",
      "step= 3200 loss= 3.09786 acc= 0.9818\n",
      "step= 3300 loss= 0.200927 acc= 0.9819\n",
      "step= 3400 loss= 1.30368 acc= 0.9842\n",
      "step= 3500 loss= 1.89895 acc= 0.982\n",
      "step= 3600 loss= 4.20349 acc= 0.9839\n",
      "step= 3700 loss= 0.700818 acc= 0.9826\n",
      "step= 3800 loss= 5.88755 acc= 0.9839\n",
      "step= 3900 loss= 2.71672 acc= 0.9847\n",
      "step= 4000 loss= 3.1661 acc= 0.9837\n",
      "step= 4100 loss= 2.41092 acc= 0.9855\n",
      "step= 4200 loss= 0.365432 acc= 0.9843\n",
      "step= 4300 loss= 4.45108 acc= 0.9849\n",
      "step= 4400 loss= 2.70948 acc= 0.9859\n",
      "step= 4500 loss= 6.44851 acc= 0.9852\n",
      "step= 4600 loss= 7.81196 acc= 0.985\n",
      "step= 4700 loss= 1.70226 acc= 0.9855\n",
      "step= 4800 loss= 15.457 acc= 0.9849\n",
      "step= 4900 loss= 1.26979 acc= 0.986\n",
      "step= 5000 loss= 6.68028 acc= 0.9858\n",
      "step= 5100 loss= 9.25035 acc= 0.9857\n",
      "step= 5200 loss= 0.367775 acc= 0.986\n",
      "step= 5300 loss= 1.11809 acc= 0.986\n",
      "step= 5400 loss= 4.14943 acc= 0.9873\n",
      "step= 5500 loss= 0.740231 acc= 0.9869\n",
      "step= 5600 loss= 0.351511 acc= 0.9864\n",
      "step= 5700 loss= 1.54925 acc= 0.9882\n",
      "step= 5800 loss= 3.15121 acc= 0.9875\n",
      "step= 5900 loss= 0.750856 acc= 0.9872\n",
      "step= 6000 loss= 0.760194 acc= 0.9858\n",
      "step= 6100 loss= 1.22515 acc= 0.9872\n",
      "step= 6200 loss= 1.67233 acc= 0.9866\n",
      "step= 6300 loss= 7.04297 acc= 0.9891\n",
      "step= 6400 loss= 3.87661 acc= 0.9879\n",
      "step= 6500 loss= 7.3614 acc= 0.9877\n",
      "step= 6600 loss= 0.277176 acc= 0.9871\n",
      "step= 6700 loss= 7.15179 acc= 0.988\n",
      "step= 6800 loss= 0.716822 acc= 0.9886\n",
      "step= 6900 loss= 0.431629 acc= 0.9887\n",
      "step= 7000 loss= 8.84609 acc= 0.9891\n",
      "step= 7100 loss= 0.503129 acc= 0.988\n",
      "step= 7200 loss= 3.63754 acc= 0.9881\n",
      "step= 7300 loss= 1.52468 acc= 0.9872\n",
      "step= 7400 loss= 5.13295 acc= 0.9881\n",
      "step= 7500 loss= 1.1043 acc= 0.9891\n",
      "step= 7600 loss= 4.54748 acc= 0.9904\n",
      "step= 7700 loss= 0.24641 acc= 0.9881\n",
      "step= 7800 loss= 0.768885 acc= 0.9897\n",
      "step= 7900 loss= 2.15445 acc= 0.9884\n",
      "step= 8000 loss= 0.195638 acc= 0.9896\n",
      "step= 8100 loss= 1.15282 acc= 0.9889\n",
      "step= 8200 loss= 0.928676 acc= 0.9894\n",
      "step= 8300 loss= 1.00701 acc= 0.9895\n",
      "step= 8400 loss= 1.44974 acc= 0.9896\n",
      "step= 8500 loss= 0.989205 acc= 0.9892\n",
      "step= 8600 loss= 0.0386515 acc= 0.9896\n",
      "step= 8700 loss= 0.217586 acc= 0.9891\n",
      "step= 8800 loss= 0.812773 acc= 0.9895\n",
      "step= 8900 loss= 0.754731 acc= 0.9902\n",
      "step= 9000 loss= 0.300084 acc= 0.9894\n",
      "step= 9100 loss= 0.847866 acc= 0.989\n",
      "step= 9200 loss= 1.79838 acc= 0.9898\n",
      "step= 9300 loss= 1.83077 acc= 0.9892\n",
      "step= 9400 loss= 2.6881 acc= 0.9903\n",
      "step= 9500 loss= 0.0952809 acc= 0.9886\n",
      "step= 9600 loss= 0.0909055 acc= 0.9897\n",
      "step= 9700 loss= 0.0990213 acc= 0.9895\n",
      "step= 9800 loss= 0.583003 acc= 0.9896\n",
      "step= 9900 loss= 0.114391 acc= 0.9887\n",
      "正解率= 0.9886\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "# MNISTの手書き画像データを読み込む\n",
    "mnist = input_data.read_data_sets(\"mnist/\", one_hot=True)\n",
    "\n",
    "pixels = 28 * 28\n",
    "nums = 10\n",
    "\n",
    "# プレースホルダを定義\n",
    "x  = tf.placeholder(tf.float32, shape=(None, pixels), name=\"x\")\n",
    "y_ = tf.placeholder(tf.float32, shape=(None, nums), name=\"y_\")\n",
    "\n",
    "# 重みとバイアスを初期化する関数\n",
    "def weight_variable(name, shape):\n",
    "    W_init = tf.truncated_normal(shape, stddev=0.1)\n",
    "    W = tf.Variable(W_init, name=\"W_\"+name)\n",
    "    return W\n",
    "def bias_variable(name, size):\n",
    "    b_init = tf.constant(0.1, shape=[size])\n",
    "    b = tf.Variable(b_init, name=\"b_\"+name)\n",
    "    return b\n",
    "\n",
    "# 畳み込みを行う関数\n",
    "def conv2d(x, W):\n",
    "    return tf.nn.conv2d(x, W, strides=[1,1,1,1], padding='SAME')\n",
    "# 最大プーリングを行う関数\n",
    "def max_pool(x):\n",
    "    return tf.nn.max_pool(x, ksize=[1,2,2,1],\n",
    "        strides=[1,2,2,1], padding='SAME')\n",
    "\n",
    "# 畳み込み層1\n",
    "with tf.name_scope('conv1') as scope:\n",
    "    W_conv1 = weight_variable('conv1', [5, 5, 1, 32])\n",
    "    b_conv1 = bias_variable('conv1', 32)\n",
    "    x_image = tf.reshape(x, [-1, 28, 28, 1])\n",
    "    h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)\n",
    "\n",
    "# プーリング層1\n",
    "with tf.name_scope('pool1') as scope:\n",
    "    h_pool1 = max_pool(h_conv1)\n",
    "\n",
    "# 畳み込み層2\n",
    "with tf.name_scope('conv2') as scope:\n",
    "    W_conv2 = weight_variable('conv2', [5, 5, 32, 64])\n",
    "    b_conv2 = bias_variable('conv2', 64)\n",
    "    h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\n",
    "\n",
    "# プーリング層2\n",
    "with tf.name_scope('pool2') as scope:\n",
    "    h_pool2 = max_pool(h_conv2)\n",
    "\n",
    "# 全結合レイヤー\n",
    "with tf.name_scope('fully_connected') as scope:\n",
    "    n = 7 * 7 * 64\n",
    "    W_fc = weight_variable('fc', [n, 1024])\n",
    "    b_fc = bias_variable('fc', 1024)\n",
    "    h_pool2_flat = tf.reshape(h_pool2, [-1, n])\n",
    "    h_fc = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc) + b_fc)\n",
    "\n",
    "# ドロップアウト(過剰適合)を排除\n",
    "with tf.name_scope('dropout') as scope:\n",
    "    keep_prob = tf.placeholder(tf.float32)\n",
    "    h_fc_drop = tf.nn.dropout(h_fc, keep_prob)\n",
    "\n",
    "# 読み出し層\n",
    "with tf.name_scope('readout') as scope:\n",
    "    W_fc2 = weight_variable('fc2', [1024, 10])\n",
    "    b_fc2 = bias_variable('fc2', 10)\n",
    "    y_conv = tf.nn.softmax(tf.matmul(h_fc_drop, W_fc2) + b_fc2)\n",
    "\n",
    "# モデルの学習\n",
    "with tf.name_scope('loss') as scope:\n",
    "    cross_entoropy = -tf.reduce_sum(y_ * tf.log(y_conv))\n",
    "with tf.name_scope('training') as scope:\n",
    "    optimizer = tf.train.AdamOptimizer(1e-4)\n",
    "    train_step = optimizer.minimize(cross_entoropy)\n",
    "\n",
    "# モデルの評価\n",
    "with tf.name_scope('predict') as scope:\n",
    "    predict_step = tf.equal(tf.argmax(y_conv, 1), tf.argmax(y_,1))\n",
    "    accuracy_step =tf.reduce_mean(tf.cast(predict_step, tf.float32))\n",
    "\n",
    "# feed_dictの設定\n",
    "def set_feed(images, labels, prob):\n",
    "    return {x: images, y_:labels, keep_prob: prob}\n",
    "\n",
    "# セッションを開始\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    # TensorBoardへの書き込み準備\n",
    "    tw = tf.summary.FileWriter('log_dir', graph=sess.graph)\n",
    "    # テスト用のフィールドを生成\n",
    "    test_fd = set_feed(mnist.test.images, mnist.test.labels, 1)\n",
    "    # 訓練を開始\n",
    "    for step in range(10000):\n",
    "        batch = mnist.train.next_batch(50)\n",
    "        fd = set_feed(batch[0], batch[1], 0.5)\n",
    "        _, loss = sess.run([train_step, cross_entoropy], feed_dict=fd)\n",
    "        if step % 100 == 0:\n",
    "            acc = sess.run(accuracy_step, feed_dict=test_fd)\n",
    "            print(\"step=\", step, \"loss=\", loss, \"acc=\", acc)\n",
    "    # 最終結果を表示\n",
    "    acc = sess.run(accuracy_step, feed_dict=test_fd)\n",
    "    print(\"正解率=\", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
